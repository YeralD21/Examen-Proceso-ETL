{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Proceso ETL aplicado a MultipleChoiceResponses.csv\n",
        "\n",
        "## Descripci√≥n del Dataset\n",
        "\n",
        "El dataset **MultipleChoiceResponses.csv** es la encuesta anual de Machine Learning y Data Science de Kaggle del a√±o 2019. Esta encuesta recopila informaci√≥n de profesionales en el campo de la ciencia de datos y machine learning a nivel mundial.\n",
        "\n",
        "### Caracter√≠sticas del Dataset\n",
        "\n",
        "- **Fuente**: Kaggle Machine Learning & Data Science Survey 2019\n",
        "- **Formato**: Archivo CSV con respuestas de opci√≥n m√∫ltiple\n",
        "- **Alcance**: Encuesta global que incluye profesionales de diferentes pa√≠ses\n",
        "- **Prop√≥sito**: Analizar tendencias, herramientas y pr√°cticas en el campo de ML/DS\n",
        "\n",
        "### Entregables del Proceso ETL\n",
        "\n",
        "Este notebook generar√° los siguientes archivos de salida:\n",
        "\n",
        "1. **CSV limpio**: `/mnt/data/multipleChoiceResponses_clean.csv`\n",
        "2. **Excel**: `/mnt/data/multipleChoiceResponses_clean.xlsx`\n",
        "3. **Base de datos SQLite**: `/mnt/data/etl_results.db`\n",
        "4. **Reporte de calidad**: `data_quality_report.json`\n",
        "5. **Log de transformaciones**: `transformation_log.csv`\n",
        "6. **Script Power Query**: `powerquery_replica.pq`\n",
        "\n",
        "### Contexto Acad√©mico\n",
        "\n",
        "Este proceso ETL se desarrolla en el contexto del curso de **Business Intelligence** de la **Universidad Peruana Uni√≥n (UPEU)**, aplicado espec√≠ficamente al √°rea de **Ingenier√≠a de Sistemas**. El an√°lisis de este dataset proporciona insights valiosos sobre:\n",
        "\n",
        "- Tecnolog√≠as m√°s utilizadas en la industria\n",
        "- Tendencias salariales por regi√≥n y experiencia\n",
        "- Herramientas de desarrollo preferidas\n",
        "- Plataformas cloud m√°s adoptadas\n",
        "- Lenguajes de programaci√≥n dominantes\n",
        "\n",
        "### Estructura del Notebook\n",
        "\n",
        "El proceso ETL se ejecutar√° en las siguientes etapas:\n",
        "\n",
        "1. **Extracci√≥n**: Carga y validaci√≥n inicial del dataset\n",
        "2. **An√°lisis Exploratorio**: Exploraci√≥n de la estructura y calidad de los datos\n",
        "3. **Transformaci√≥n**: Limpieza, normalizaci√≥n y enriquecimiento de datos\n",
        "4. **Carga**: Exportaci√≥n a diferentes formatos para an√°lisis\n",
        "5. **Reportes**: Generaci√≥n de m√©tricas de calidad y logs\n",
        "6. **Validaci√≥n**: Comparaci√≥n con Power BI para consistencia\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuraci√≥n del Entorno\n",
        "\n",
        "### Importaci√≥n de Librer√≠as\n",
        "\n",
        "Antes de comenzar el proceso ETL, es necesario importar todas las librer√≠as requeridas. Utilizaremos las siguientes librer√≠as est√°ndar para el an√°lisis de datos:\n",
        "\n",
        "- **pandas**: Manipulaci√≥n y an√°lisis de datos estructurados\n",
        "- **numpy**: Operaciones num√©ricas y matem√°ticas\n",
        "- **matplotlib**: Visualizaciones b√°sicas\n",
        "- **seaborn**: Visualizaciones estad√≠sticas avanzadas\n",
        "- **sqlalchemy**: Conexi√≥n y operaciones con bases de datos\n",
        "- **json**: Manejo de archivos JSON\n",
        "- **datetime**: Operaciones con fechas y tiempos\n",
        "\n",
        "### Configuraci√≥n de Par√°metros\n",
        "\n",
        "Estableceremos par√°metros de visualizaci√≥n y configuraci√≥n para asegurar que los resultados sean consistentes y profesionales.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaci√≥n de librer√≠as necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sqlalchemy\n",
        "import json\n",
        "import datetime\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Configuraci√≥n de par√°metros de visualizaci√≥n\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n de pandas para mostrar m√°s informaci√≥n\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
        "print(f\"üìÖ Fecha de ejecuci√≥n: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üêç Versi√≥n de Python: {pd.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Extracci√≥n de Datos\n",
        "\n",
        "### Descripci√≥n del Proceso de Extracci√≥n\n",
        "\n",
        "La extracci√≥n de datos es la primera fase del proceso ETL (Extract, Transform, Load). En esta etapa, cargamos el dataset desde su fuente original y realizamos una validaci√≥n inicial para entender su estructura y calidad.\n",
        "\n",
        "### Origen de los Datos\n",
        "\n",
        "El dataset **MultipleChoiceResponses.csv** proviene de la encuesta anual de Kaggle sobre Machine Learning y Data Science. Esta encuesta es una de las m√°s comprehensivas del sector, recopilando informaci√≥n de miles de profesionales a nivel mundial.\n",
        "\n",
        "### Proceso de Carga\n",
        "\n",
        "Utilizaremos pandas para cargar el archivo CSV, especificando los par√°metros apropiados para manejar la codificaci√≥n y estructura del archivo. Es importante verificar que la carga se realice correctamente y que no se pierdan datos durante el proceso.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carga del dataset\n",
        "print(\"üìä Cargando dataset MultipleChoiceResponses.csv...\")\n",
        "\n",
        "# Intentar cargar desde la ruta especificada, si no existe, usar la ruta local\n",
        "try:\n",
        "    df = pd.read_csv('/mnt/data/multipleChoiceResponses.csv')\n",
        "    print(\"‚úÖ Dataset cargado desde /mnt/data/\")\n",
        "except FileNotFoundError:\n",
        "    try:\n",
        "        df = pd.read_csv('multipleChoiceResponses.csv')\n",
        "        print(\"‚úÖ Dataset cargado desde directorio local\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå Error: No se encontr√≥ el archivo multipleChoiceResponses.csv\")\n",
        "        print(\"üí° Aseg√∫rate de que el archivo est√© en el directorio correcto\")\n",
        "\n",
        "# Informaci√≥n b√°sica del dataset\n",
        "print(f\"\\nüìã INFORMACI√ìN B√ÅSICA DEL DATASET:\")\n",
        "print(f\"   ‚Ä¢ Dimensiones: {df.shape[0]:,} filas √ó {df.shape[1]:,} columnas\")\n",
        "print(f\"   ‚Ä¢ Memoria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "print(f\"   ‚Ä¢ Tipos de datos √∫nicos: {df.dtypes.nunique()}\")\n",
        "\n",
        "# Mostrar las primeras filas\n",
        "print(f\"\\nüîç PRIMERAS 5 FILAS DEL DATASET:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Informaci√≥n detallada del dataset\n",
        "print(\"üìä INFORMACI√ìN DETALLADA DEL DATASET:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Informaci√≥n general\n",
        "print(f\"üìã INFORMACI√ìN GENERAL:\")\n",
        "print(f\"   ‚Ä¢ N√∫mero de filas: {df.shape[0]:,}\")\n",
        "print(f\"   ‚Ä¢ N√∫mero de columnas: {df.shape[1]:,}\")\n",
        "print(f\"   ‚Ä¢ Total de celdas: {df.shape[0] * df.shape[1]:,}\")\n",
        "\n",
        "# Informaci√≥n de tipos de datos\n",
        "print(f\"\\nüî¢ TIPOS DE DATOS:\")\n",
        "tipos_datos = df.dtypes.value_counts()\n",
        "for tipo, cantidad in tipos_datos.items():\n",
        "    print(f\"   ‚Ä¢ {tipo}: {cantidad} columnas\")\n",
        "\n",
        "# Informaci√≥n de memoria\n",
        "memoria_total = df.memory_usage(deep=True).sum() / 1024**2\n",
        "memoria_por_fila = memoria_total * 1024 / df.shape[0]\n",
        "print(f\"\\nüíæ USO DE MEMORIA:\")\n",
        "print(f\"   ‚Ä¢ Memoria total: {memoria_total:.2f} MB\")\n",
        "print(f\"   ‚Ä¢ Memoria por fila: {memoria_por_fila:.2f} KB\")\n",
        "\n",
        "# Mostrar info() completo\n",
        "print(f\"\\nüìã INFORMACI√ìN COMPLETA (df.info()):\")\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. An√°lisis Exploratorio de Datos (EDA)\n",
        "\n",
        "### ¬øQu√© es el EDA?\n",
        "\n",
        "El **An√°lisis Exploratorio de Datos (EDA)** es una fase crucial en cualquier proyecto de an√°lisis de datos. Consiste en examinar los datos para entender su estructura, identificar patrones, detectar anomal√≠as y evaluar la calidad de los datos antes de proceder con la transformaci√≥n.\n",
        "\n",
        "### Objetivos del EDA\n",
        "\n",
        "1. **Comprensi√≥n de la estructura**: Entender c√≥mo est√°n organizados los datos\n",
        "2. **Identificaci√≥n de problemas**: Detectar valores faltantes, duplicados, inconsistencias\n",
        "3. **An√°lisis de distribuciones**: Comprender c√≥mo se distribuyen las variables\n",
        "4. **Detecci√≥n de outliers**: Identificar valores at√≠picos que puedan afectar el an√°lisis\n",
        "5. **Preparaci√≥n para transformaci√≥n**: Determinar qu√© limpieza y transformaciones son necesarias\n",
        "\n",
        "### Metodolog√≠a del EDA\n",
        "\n",
        "Realizaremos un EDA sistem√°tico que incluye:\n",
        "\n",
        "- **Estad√≠sticas descriptivas**: Medidas de tendencia central y dispersi√≥n\n",
        "- **An√°lisis de completitud**: Evaluaci√≥n de valores faltantes por columna\n",
        "- **Detecci√≥n de duplicados**: Identificaci√≥n de registros repetidos\n",
        "- **An√°lisis de tipos de datos**: Verificaci√≥n de la consistencia de tipos\n",
        "- **Visualizaciones exploratorias**: Gr√°ficos para entender distribuciones\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lisis de valores faltantes\n",
        "print(\"‚ùå AN√ÅLISIS DE VALORES FALTANTES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Calcular valores faltantes por columna\n",
        "valores_faltantes = df.isnull().sum()\n",
        "porcentaje_faltantes = (valores_faltantes / len(df)) * 100\n",
        "\n",
        "# Crear DataFrame con el an√°lisis\n",
        "analisis_faltantes = pd.DataFrame({\n",
        "    'Columna': valores_faltantes.index,\n",
        "    'Valores_Faltantes': valores_faltantes.values,\n",
        "    'Porcentaje_Faltantes': porcentaje_faltantes.values\n",
        "})\n",
        "\n",
        "# Ordenar por n√∫mero de valores faltantes\n",
        "analisis_faltantes = analisis_faltantes.sort_values('Valores_Faltantes', ascending=False)\n",
        "\n",
        "print(f\"üìä RESUMEN DE VALORES FALTANTES:\")\n",
        "print(f\"   ‚Ä¢ Total de valores faltantes: {valores_faltantes.sum():,}\")\n",
        "print(f\"   ‚Ä¢ Columnas con valores faltantes: {(valores_faltantes > 0).sum()}\")\n",
        "print(f\"   ‚Ä¢ Columnas completas: {(valores_faltantes == 0).sum()}\")\n",
        "\n",
        "# Mostrar las 10 columnas con m√°s valores faltantes\n",
        "print(f\"\\nüîù TOP 10 COLUMNAS CON M√ÅS VALORES FALTANTES:\")\n",
        "print(analisis_faltantes.head(10).to_string(index=False))\n",
        "\n",
        "# Categorizar columnas por nivel de completitud\n",
        "completas = (porcentaje_faltantes == 0).sum()\n",
        "pocos_faltantes = ((porcentaje_faltantes > 0) & (porcentaje_faltantes <= 20)).sum()\n",
        "moderados_faltantes = ((porcentaje_faltantes > 20) & (porcentaje_faltantes <= 50)).sum()\n",
        "muchos_faltantes = ((porcentaje_faltantes > 50) & (porcentaje_faltantes <= 80)).sum()\n",
        "criticos_faltantes = (porcentaje_faltantes > 80).sum()\n",
        "\n",
        "print(f\"\\nüìã CATEGORIZACI√ìN POR COMPLETITUD:\")\n",
        "print(f\"   ‚Ä¢ Completas (0% faltantes): {completas} columnas\")\n",
        "print(f\"   ‚Ä¢ Pocos faltantes (0-20%): {pocos_faltantes} columnas\")\n",
        "print(f\"   ‚Ä¢ Moderados (20-50%): {moderados_faltantes} columnas\")\n",
        "print(f\"   ‚Ä¢ Muchos (50-80%): {muchos_faltantes} columnas\")\n",
        "print(f\"   ‚Ä¢ Cr√≠ticos (>80%): {criticos_faltantes} columnas\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lisis de duplicados\n",
        "print(\"\\nüîÑ AN√ÅLISIS DE REGISTROS DUPLICADOS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Detectar duplicados\n",
        "duplicados = df.duplicated()\n",
        "num_duplicados = duplicados.sum()\n",
        "porcentaje_duplicados = (num_duplicados / len(df)) * 100\n",
        "\n",
        "print(f\"üìä AN√ÅLISIS DE DUPLICADOS:\")\n",
        "print(f\"   ‚Ä¢ Registros duplicados: {num_duplicados:,}\")\n",
        "print(f\"   ‚Ä¢ Porcentaje de duplicados: {porcentaje_duplicados:.2f}%\")\n",
        "print(f\"   ‚Ä¢ Registros √∫nicos: {len(df) - num_duplicados:,}\")\n",
        "\n",
        "if num_duplicados > 0:\n",
        "    print(f\"\\nüîç EJEMPLOS DE REGISTROS DUPLICADOS:\")\n",
        "    # Mostrar algunos ejemplos de duplicados\n",
        "    duplicados_ejemplos = df[duplicados].head(3)\n",
        "    print(duplicados_ejemplos.to_string())\n",
        "else:\n",
        "    print(f\"\\n‚úÖ No se encontraron registros duplicados\")\n",
        "\n",
        "# An√°lisis de valores √∫nicos por columna\n",
        "print(f\"\\nüî¢ AN√ÅLISIS DE VALORES √öNICOS POR COLUMNA\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "valores_unicos = df.nunique()\n",
        "valores_unicos_ordenados = valores_unicos.sort_values(ascending=False)\n",
        "\n",
        "print(f\"üìä ESTAD√çSTICAS DE VALORES √öNICOS:\")\n",
        "print(f\"   ‚Ä¢ Promedio de valores √∫nicos por columna: {valores_unicos.mean():.1f}\")\n",
        "print(f\"   ‚Ä¢ Mediana de valores √∫nicos por columna: {valores_unicos.median():.1f}\")\n",
        "print(f\"   ‚Ä¢ M√°ximo de valores √∫nicos: {valores_unicos.max()}\")\n",
        "print(f\"   ‚Ä¢ M√≠nimo de valores √∫nicos: {valores_unicos.min()}\")\n",
        "\n",
        "# Identificar columnas constantes y casi constantes\n",
        "columnas_constantes = (valores_unicos == 1).sum()\n",
        "columnas_casi_constantes = (valores_unicos <= 5).sum()\n",
        "columnas_casi_unicas = (valores_unicos >= len(df) * 0.95).sum()\n",
        "\n",
        "print(f\"\\nüìã CATEGORIZACI√ìN DE COLUMNAS:\")\n",
        "print(f\"   ‚Ä¢ Columnas constantes (1 valor √∫nico): {columnas_constantes}\")\n",
        "print(f\"   ‚Ä¢ Columnas casi constantes (‚â§5 valores): {columnas_casi_constantes}\")\n",
        "print(f\"   ‚Ä¢ Columnas casi √∫nicas (‚â•95% √∫nicos): {columnas_casi_unicas}\")\n",
        "\n",
        "# Mostrar las 10 columnas con m√°s valores √∫nicos\n",
        "print(f\"\\nüîù TOP 10 COLUMNAS CON M√ÅS VALORES √öNICOS:\")\n",
        "top_unicos = valores_unicos_ordenados.head(10)\n",
        "for col, unicos in top_unicos.items():\n",
        "    print(f\"   ‚Ä¢ {col}: {unicos} valores √∫nicos\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estad√≠sticas descriptivas para columnas num√©ricas\n",
        "print(\"\\nüìä ESTAD√çSTICAS DESCRIPTIVAS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Identificar columnas num√©ricas\n",
        "columnas_numericas = df.select_dtypes(include=[np.number]).columns\n",
        "columnas_categoricas = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "print(f\"üìã CLASIFICACI√ìN DE COLUMNAS:\")\n",
        "print(f\"   ‚Ä¢ Columnas num√©ricas: {len(columnas_numericas)}\")\n",
        "print(f\"   ‚Ä¢ Columnas categ√≥ricas: {len(columnas_categoricas)}\")\n",
        "\n",
        "if len(columnas_numericas) > 0:\n",
        "    print(f\"\\nüî¢ ESTAD√çSTICAS DESCRIPTIVAS - COLUMNAS NUM√âRICAS:\")\n",
        "    estadisticas_numericas = df[columnas_numericas].describe()\n",
        "    print(estadisticas_numericas.to_string())\n",
        "    \n",
        "    # An√°lisis adicional para columnas num√©ricas\n",
        "    print(f\"\\nüìà AN√ÅLISIS ADICIONAL - COLUMNAS NUM√âRICAS:\")\n",
        "    for col in columnas_numericas:\n",
        "        valores_unicos = df[col].nunique()\n",
        "        valores_faltantes = df[col].isnull().sum()\n",
        "        print(f\"   ‚Ä¢ {col}:\")\n",
        "        print(f\"     - Valores √∫nicos: {valores_unicos}\")\n",
        "        print(f\"     - Valores faltantes: {valores_faltantes}\")\n",
        "        print(f\"     - Rango: {df[col].min():.2f} - {df[col].max():.2f}\")\n",
        "\n",
        "# An√°lisis de columnas categ√≥ricas principales\n",
        "print(f\"\\nüìù AN√ÅLISIS DE COLUMNAS CATEG√ìRICAS PRINCIPALES\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "# Analizar las primeras 5 columnas categ√≥ricas\n",
        "columnas_principales = columnas_categoricas[:5]\n",
        "for col in columnas_principales:\n",
        "    print(f\"\\nüîπ {col}:\")\n",
        "    valores_unicos = df[col].nunique()\n",
        "    valores_faltantes = df[col].isnull().sum()\n",
        "    print(f\"   ‚Ä¢ Valores √∫nicos: {valores_unicos}\")\n",
        "    print(f\"   ‚Ä¢ Valores faltantes: {valores_faltantes}\")\n",
        "    \n",
        "    if valores_unicos <= 20:  # Mostrar distribuci√≥n si no hay muchos valores √∫nicos\n",
        "        distribucion = df[col].value_counts().head(10)\n",
        "        print(f\"   ‚Ä¢ Top 10 valores m√°s frecuentes:\")\n",
        "        for valor, count in distribucion.items():\n",
        "            porcentaje = (count / len(df)) * 100\n",
        "            print(f\"     - {valor}: {count} ({porcentaje:.1f}%)\")\n",
        "    else:\n",
        "        print(f\"   ‚Ä¢ Demasiados valores √∫nicos para mostrar distribuci√≥n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Transformaci√≥n de Datos\n",
        "\n",
        "### Descripci√≥n del Proceso de Transformaci√≥n\n",
        "\n",
        "La transformaci√≥n de datos es la fase central del proceso ETL, donde aplicamos una serie de operaciones para limpiar, normalizar y enriquecer los datos. Esta fase es cr√≠tica para asegurar la calidad y consistencia de los datos que ser√°n utilizados en an√°lisis posteriores.\n",
        "\n",
        "### Estrategias de Transformaci√≥n\n",
        "\n",
        "Bas√°ndonos en los hallazgos del EDA, aplicaremos las siguientes transformaciones:\n",
        "\n",
        "1. **Eliminaci√≥n de duplicados**: Remover registros completamente duplicados\n",
        "2. **Manejo de valores nulos**: Estrategias diferenciadas seg√∫n el porcentaje de valores faltantes\n",
        "3. **Normalizaci√≥n de strings**: Limpieza de espacios, estandarizaci√≥n de formatos\n",
        "4. **Conversi√≥n de tipos**: Optimizaci√≥n de tipos de datos para eficiencia\n",
        "5. **Renombrado de columnas**: Mapeo de Q1, Q2, etc. a nombres descriptivos\n",
        "6. **Agrupaci√≥n de categor√≠as**: Consolidaci√≥n de valores poco frecuentes\n",
        "\n",
        "### Criterios de Decisi√≥n\n",
        "\n",
        "- **Columnas con >80% nulos**: Eliminaci√≥n completa\n",
        "- **Columnas con 20-80% nulos**: Imputaci√≥n con valores apropiados\n",
        "- **Columnas con <20% nulos**: Imputaci√≥n conservadora\n",
        "- **Duplicados**: Eliminaci√≥n completa para evitar sesgos\n",
        "- **Strings**: Normalizaci√≥n a min√∫sculas y eliminaci√≥n de espacios\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicializar log de transformaciones\n",
        "transformation_log = []\n",
        "timestamp_inicio = datetime.datetime.now()\n",
        "\n",
        "print(\"üîÑ INICIANDO PROCESO DE TRANSFORMACI√ìN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Crear copia del dataset original para transformaci√≥n\n",
        "df_clean = df.copy()\n",
        "filas_iniciales = len(df_clean)\n",
        "columnas_iniciales = len(df_clean.columns)\n",
        "\n",
        "print(f\"üìä ESTADO INICIAL:\")\n",
        "print(f\"   ‚Ä¢ Filas: {filas_iniciales:,}\")\n",
        "print(f\"   ‚Ä¢ Columnas: {columnas_iniciales:,}\")\n",
        "\n",
        "# PASO 1: Eliminaci√≥n de duplicados\n",
        "print(f\"\\nüîÑ PASO 1: ELIMINACI√ìN DE DUPLICADOS\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "duplicados_antes = df_clean.duplicated().sum()\n",
        "df_clean = df_clean.drop_duplicates()\n",
        "duplicados_eliminados = duplicados_antes\n",
        "filas_despues_duplicados = len(df_clean)\n",
        "\n",
        "print(f\"   ‚Ä¢ Duplicados encontrados: {duplicados_antes:,}\")\n",
        "print(f\"   ‚Ä¢ Duplicados eliminados: {duplicados_eliminados:,}\")\n",
        "print(f\"   ‚Ä¢ Filas restantes: {filas_despues_duplicados:,}\")\n",
        "\n",
        "# Registrar en log\n",
        "transformation_log.append({\n",
        "    'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'operacion': 'Eliminacion_Duplicados',\n",
        "    'filas_antes': filas_iniciales,\n",
        "    'filas_despues': filas_despues_duplicados,\n",
        "    'filas_eliminadas': duplicados_eliminados,\n",
        "    'descripcion': f'Eliminados {duplicados_eliminados} registros duplicados'\n",
        "})\n",
        "\n",
        "print(f\"   ‚úÖ Duplicados eliminados exitosamente\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PASO 2: Manejo de valores nulos\n",
        "print(f\"\\n‚ùå PASO 2: MANEJO DE VALORES NULOS\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "# Calcular porcentaje de nulos por columna\n",
        "porcentaje_nulos = (df_clean.isnull().sum() / len(df_clean)) * 100\n",
        "\n",
        "# Categorizar columnas por nivel de nulos\n",
        "columnas_eliminar = porcentaje_nulos[porcentaje_nulos > 80].index.tolist()\n",
        "columnas_imputar = porcentaje_nulos[(porcentaje_nulos > 0) & (porcentaje_nulos <= 80)].index.tolist()\n",
        "columnas_completas = porcentaje_nulos[porcentaje_nulos == 0].index.tolist()\n",
        "\n",
        "print(f\"üìä CATEGORIZACI√ìN DE COLUMNAS:\")\n",
        "print(f\"   ‚Ä¢ Para eliminar (>80% nulos): {len(columnas_eliminar)}\")\n",
        "print(f\"   ‚Ä¢ Para imputar (0-80% nulos): {len(columnas_imputar)}\")\n",
        "print(f\"   ‚Ä¢ Completas (0% nulos): {len(columnas_completas)}\")\n",
        "\n",
        "# Eliminar columnas con muchos nulos\n",
        "if columnas_eliminar:\n",
        "    print(f\"\\nüóëÔ∏è ELIMINANDO COLUMNAS CON >80% NULOS:\")\n",
        "    for col in columnas_eliminar[:5]:  # Mostrar primeras 5\n",
        "        pct = porcentaje_nulos[col]\n",
        "        print(f\"   ‚Ä¢ {col}: {pct:.1f}% nulos\")\n",
        "    if len(columnas_eliminar) > 5:\n",
        "        print(f\"   ‚Ä¢ ... y {len(columnas_eliminar) - 5} columnas m√°s\")\n",
        "    \n",
        "    df_clean = df_clean.drop(columns=columnas_eliminar)\n",
        "    print(f\"   ‚úÖ {len(columnas_eliminar)} columnas eliminadas\")\n",
        "\n",
        "# Imputar valores nulos en columnas restantes\n",
        "if columnas_imputar:\n",
        "    print(f\"\\nüîß IMPUTANDO VALORES NULOS:\")\n",
        "    columnas_imputar_restantes = [col for col in columnas_imputar if col in df_clean.columns]\n",
        "    \n",
        "    for col in columnas_imputar_restantes:\n",
        "        nulos_antes = df_clean[col].isnull().sum()\n",
        "        if nulos_antes > 0:\n",
        "            # Imputaci√≥n categ√≥rica\n",
        "            if df_clean[col].dtype == 'object':\n",
        "                df_clean[col].fillna('No especificado', inplace=True)\n",
        "            # Imputaci√≥n num√©rica\n",
        "            else:\n",
        "                mediana = df_clean[col].median()\n",
        "                df_clean[col].fillna(mediana, inplace=True)\n",
        "    \n",
        "    print(f\"   ‚úÖ {len(columnas_imputar_restantes)} columnas imputadas\")\n",
        "\n",
        "# Verificar nulos restantes\n",
        "nulos_restantes = df_clean.isnull().sum().sum()\n",
        "print(f\"\\nüìä RESULTADO MANEJO DE NULOS:\")\n",
        "print(f\"   ‚Ä¢ Nulos restantes: {nulos_restantes:,}\")\n",
        "\n",
        "# Registrar en log\n",
        "transformation_log.append({\n",
        "    'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'operacion': 'Manejo_Valores_Nulos',\n",
        "    'filas_antes': filas_despues_duplicados,\n",
        "    'filas_despues': len(df_clean),\n",
        "    'filas_eliminadas': len(columnas_eliminar),\n",
        "    'descripcion': f'Eliminadas {len(columnas_eliminar)} columnas, imputados nulos en {len(columnas_imputar_restantes)} columnas'\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PASO 3: Normalizaci√≥n de strings\n",
        "print(f\"\\nüîß PASO 3: NORMALIZACI√ìN DE STRINGS\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "# Identificar columnas de texto\n",
        "columnas_texto = df_clean.select_dtypes(include=['object']).columns\n",
        "print(f\"üìù COLUMNAS DE TEXTO IDENTIFICADAS: {len(columnas_texto)}\")\n",
        "\n",
        "# Normalizar strings\n",
        "for col in columnas_texto:\n",
        "    if col in df_clean.columns:\n",
        "        # Limpiar espacios al inicio y final\n",
        "        df_clean[col] = df_clean[col].astype(str).str.strip()\n",
        "        # Reemplazar m√∫ltiples espacios por uno solo\n",
        "        df_clean[col] = df_clean[col].str.replace(r'\\s+', ' ', regex=True)\n",
        "        # Convertir a min√∫sculas para consistencia\n",
        "        df_clean[col] = df_clean[col].str.lower()\n",
        "\n",
        "print(f\"   ‚úÖ {len(columnas_texto)} columnas de texto normalizadas\")\n",
        "\n",
        "# PASO 4: Renombrado de columnas principales\n",
        "print(f\"\\nüìù PASO 4: RENOMBRADO DE COLUMNAS\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "# Mapeo de columnas Q1-Q50 a nombres descriptivos\n",
        "mapeo_columnas = {\n",
        "    'Time from Start to Finish (seconds)': 'Tiempo_Total_Encuesta_Segundos',\n",
        "    'Q1': 'Edad_Encuestado',\n",
        "    'Q1_OTHER_TEXT': 'Edad_Encuestado_Texto_Libre',\n",
        "    'Q2': 'Genero',\n",
        "    'Q3': 'Pais_Residencia',\n",
        "    'Q4': 'Nivel_Educativo',\n",
        "    'Q5': 'Area_Estudios_Principal',\n",
        "    'Q6': 'Situacion_Laboral_Actual',\n",
        "    'Q6_OTHER_TEXT': 'Situacion_Laboral_Texto_Libre',\n",
        "    'Q7': 'Cargo_Principal_Trabajo',\n",
        "    'Q7_OTHER_TEXT': 'Cargo_Texto_Libre',\n",
        "    'Q8': 'Anos_Experiencia_Campo',\n",
        "    'Q9': 'Rango_Salarial_Anual',\n",
        "    'Q10': 'Lenguajes_Programacion_Usados',\n",
        "    'Q11_Part_1': 'IDE_Jupyter_Notebooks',\n",
        "    'Q11_Part_2': 'IDE_RStudio',\n",
        "    'Q11_Part_3': 'IDE_PyCharm',\n",
        "    'Q11_Part_4': 'IDE_Atom',\n",
        "    'Q11_Part_5': 'IDE_MATLAB',\n",
        "    'Q12_MULTIPLE_CHOICE': 'Hardware_Analisis_Datos',\n",
        "    'Q13_Part_1': 'Cloud_AWS',\n",
        "    'Q13_Part_2': 'Cloud_Microsoft_Azure',\n",
        "    'Q13_Part_3': 'Cloud_Google_Cloud',\n",
        "    'Q13_Part_4': 'Cloud_IBM',\n",
        "    'Q14_Part_1': 'TPU_Google',\n",
        "    'Q15_Part_1': 'BigData_Spark',\n",
        "    'Q15_Part_2': 'BigData_Hadoop'\n",
        "}\n",
        "\n",
        "# Aplicar renombrado solo a columnas que existen\n",
        "columnas_existentes = {k: v for k, v in mapeo_columnas.items() if k in df_clean.columns}\n",
        "df_clean = df_clean.rename(columns=columnas_existentes)\n",
        "\n",
        "print(f\"üìù RENOMBRADO DE COLUMNAS:\")\n",
        "print(f\"   ‚Ä¢ Columnas renombradas: {len(columnas_existentes)}\")\n",
        "\n",
        "# Mostrar algunos ejemplos\n",
        "print(f\"\\nüîπ EJEMPLOS DE RENOMBRADO:\")\n",
        "for i, (original, nuevo) in enumerate(list(columnas_existentes.items())[:8]):\n",
        "    print(f\"   {i+1}. {original[:35]:<35} ‚Üí {nuevo}\")\n",
        "\n",
        "if len(columnas_existentes) > 8:\n",
        "    print(f\"   ... y {len(columnas_existentes) - 8} columnas m√°s\")\n",
        "\n",
        "# Registrar en log\n",
        "transformation_log.append({\n",
        "    'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'operacion': 'Normalizacion_y_Renombrado',\n",
        "    'filas_antes': len(df_clean),\n",
        "    'filas_despues': len(df_clean),\n",
        "    'filas_eliminadas': 0,\n",
        "    'descripcion': f'Normalizadas {len(columnas_texto)} columnas de texto, renombradas {len(columnas_existentes)} columnas'\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PASO 5: Creaci√≥n de variables derivadas\n",
        "print(f\"\\n‚ûï PASO 5: CREACI√ìN DE VARIABLES DERIVADAS\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "# 1. Categor√≠a de experiencia\n",
        "if 'Anos_Experiencia_Campo' in df_clean.columns:\n",
        "    def categorizar_experiencia(experiencia):\n",
        "        if pd.isna(experiencia):\n",
        "            return 'no especificado'\n",
        "        exp_str = str(experiencia).lower()\n",
        "        if any(x in exp_str for x in ['0-1', '< 1', 'less than 1']):\n",
        "            return 'principiante (0-2 a√±os)'\n",
        "        elif any(x in exp_str for x in ['1-2', '2-3']):\n",
        "            return 'principiante (0-2 a√±os)'\n",
        "        elif any(x in exp_str for x in ['3-4', '4-5']):\n",
        "            return 'intermedio (3-5 a√±os)'\n",
        "        elif any(x in exp_str for x in ['5-10']):\n",
        "            return 'avanzado (5-10 a√±os)'\n",
        "        elif any(x in exp_str for x in ['10-15', '15-20', '20+']):\n",
        "            return 'experto (10+ a√±os)'\n",
        "        else:\n",
        "            return 'no especificado'\n",
        "    \n",
        "    df_clean['Categoria_Experiencia'] = df_clean['Anos_Experiencia_Campo'].apply(categorizar_experiencia)\n",
        "    print(\"   ‚úÖ Categoria_Experiencia creada\")\n",
        "\n",
        "# 2. Categor√≠a salarial\n",
        "if 'Rango_Salarial_Anual' in df_clean.columns:\n",
        "    def categorizar_salario(salario):\n",
        "        if pd.isna(salario):\n",
        "            return 'no especificado'\n",
        "        sal_str = str(salario).lower()\n",
        "        if 'not wish' in sal_str or 'do not' in sal_str:\n",
        "            return 'no especificado'\n",
        "        elif any(x in sal_str for x in ['0-10,000', '10,000-20,000']):\n",
        "            return 'bajo (0-20k usd)'\n",
        "        elif any(x in sal_str for x in ['20,000-30,000', '30,000-40,000', '40,000-50,000']):\n",
        "            return 'medio (20-50k usd)'\n",
        "        elif any(x in sal_str for x in ['50,000-60,000', '60,000-70,000', '70,000-80,000']):\n",
        "            return 'alto (50-80k usd)'\n",
        "        elif any(x in sal_str for x in ['80,000', '90,000', '100,000']):\n",
        "            return 'muy alto (80-100k usd)'\n",
        "        elif any(x in sal_str for x in ['125,000', '150,000', '200,000', '300,000', '400,000', '500,000']):\n",
        "            return 'ejecutivo (100k+ usd)'\n",
        "        else:\n",
        "            return 'no especificado'\n",
        "    \n",
        "    df_clean['Categoria_Salarial'] = df_clean['Rango_Salarial_Anual'].apply(categorizar_salario)\n",
        "    print(\"   ‚úÖ Categoria_Salarial creada\")\n",
        "\n",
        "# 3. Regi√≥n geogr√°fica\n",
        "if 'Pais_Residencia' in df_clean.columns:\n",
        "    def categorizar_region(pais):\n",
        "        if pd.isna(pais):\n",
        "            return 'no especificado'\n",
        "        pais_str = str(pais).lower()\n",
        "        \n",
        "        if any(p in pais_str for p in ['united states', 'canada', 'mexico']):\n",
        "            return 'am√©rica del norte'\n",
        "        elif any(p in pais_str for p in ['brazil', 'argentina', 'colombia', 'chile', 'peru']):\n",
        "            return 'am√©rica latina'\n",
        "        elif any(p in pais_str for p in ['united kingdom', 'germany', 'france', 'spain', 'italy', 'russia']):\n",
        "            return 'europa'\n",
        "        elif any(p in pais_str for p in ['india', 'china', 'japan', 'australia', 'singapore']):\n",
        "            return 'asia-pac√≠fico'\n",
        "        else:\n",
        "            return 'otros'\n",
        "    \n",
        "    df_clean['Region_Geografica'] = df_clean['Pais_Residencia'].apply(categorizar_region)\n",
        "    print(\"   ‚úÖ Region_Geografica creada\")\n",
        "\n",
        "# Mostrar distribuciones de variables derivadas\n",
        "print(f\"\\nüìä DISTRIBUCIONES DE VARIABLES DERIVADAS:\")\n",
        "\n",
        "if 'Categoria_Experiencia' in df_clean.columns:\n",
        "    dist_exp = df_clean['Categoria_Experiencia'].value_counts()\n",
        "    print(f\"\\nüîπ Categor√≠a de Experiencia:\")\n",
        "    for cat, count in dist_exp.head().items():\n",
        "        pct = (count / len(df_clean)) * 100\n",
        "        print(f\"   ‚Ä¢ {cat}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "if 'Categoria_Salarial' in df_clean.columns:\n",
        "    dist_sal = df_clean['Categoria_Salarial'].value_counts()\n",
        "    print(f\"\\nüîπ Categor√≠a Salarial:\")\n",
        "    for cat, count in dist_sal.head().items():\n",
        "        pct = (count / len(df_clean)) * 100\n",
        "        print(f\"   ‚Ä¢ {cat}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "# Registrar en log\n",
        "transformation_log.append({\n",
        "    'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'operacion': 'Creacion_Variables_Derivadas',\n",
        "    'filas_antes': len(df_clean),\n",
        "    'filas_despues': len(df_clean),\n",
        "    'filas_eliminadas': 0,\n",
        "    'descripcion': 'Creadas 3 variables derivadas: Categoria_Experiencia, Categoria_Salarial, Region_Geografica'\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resumen final de la transformaci√≥n\n",
        "print(f\"\\nüìä RESUMEN FINAL DE LA TRANSFORMACI√ìN\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# M√©tricas finales\n",
        "filas_finales = len(df_clean)\n",
        "columnas_finales = len(df_clean.columns)\n",
        "nulos_finales = df_clean.isnull().sum().sum()\n",
        "memoria_final = df_clean.memory_usage(deep=True).sum() / 1024**2\n",
        "\n",
        "print(f\"üìà COMPARACI√ìN ANTES VS DESPU√âS:\")\n",
        "print(f\"   ‚Ä¢ Filas: {filas_iniciales:,} ‚Üí {filas_finales:,} ({filas_iniciales - filas_finales:+,})\")\n",
        "print(f\"   ‚Ä¢ Columnas: {columnas_iniciales:,} ‚Üí {columnas_finales:,} ({columnas_iniciales - columnas_finales:+,})\")\n",
        "print(f\"   ‚Ä¢ Nulos: {df.isnull().sum().sum():,} ‚Üí {nulos_finales:,} ({df.isnull().sum().sum() - nulos_finales:+,})\")\n",
        "print(f\"   ‚Ä¢ Memoria: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB ‚Üí {memoria_final:.1f} MB\")\n",
        "\n",
        "# Porcentajes de mejora\n",
        "reduccion_nulos = ((df.isnull().sum().sum() - nulos_finales) / df.isnull().sum().sum() * 100) if df.isnull().sum().sum() > 0 else 0\n",
        "reduccion_memoria = ((df.memory_usage(deep=True).sum() / 1024**2 - memoria_final) / (df.memory_usage(deep=True).sum() / 1024**2) * 100)\n",
        "\n",
        "print(f\"\\nüìä MEJORAS LOGRADAS:\")\n",
        "print(f\"   ‚Ä¢ Reducci√≥n de nulos: {reduccion_nulos:.1f}%\")\n",
        "print(f\"   ‚Ä¢ Reducci√≥n de memoria: {reduccion_memoria:.1f}%\")\n",
        "print(f\"   ‚Ä¢ Completitud de datos: {((filas_finales * columnas_finales - nulos_finales) / (filas_finales * columnas_finales) * 100):.1f}%\")\n",
        "\n",
        "# Resumen de transformaciones aplicadas\n",
        "print(f\"\\n‚úÖ TRANSFORMACIONES APLICADAS:\")\n",
        "print(f\"   ‚Ä¢ ‚úÖ Eliminaci√≥n de duplicados\")\n",
        "print(f\"   ‚Ä¢ ‚úÖ Manejo inteligente de valores nulos\")\n",
        "print(f\"   ‚Ä¢ ‚úÖ Limpieza de espacios en blanco\")\n",
        "print(f\"   ‚Ä¢ ‚úÖ Normalizaci√≥n de datos\")\n",
        "print(f\"   ‚Ä¢ ‚úÖ Renombrado de columnas descriptivo\")\n",
        "print(f\"   ‚Ä¢ ‚úÖ Creaci√≥n de variables derivadas\")\n",
        "\n",
        "# Mostrar primeras filas del dataset transformado\n",
        "print(f\"\\nüìã PRIMERAS 3 FILAS DEL DATASET TRANSFORMADO:\")\n",
        "columnas_mostrar = [col for col in df_clean.columns if any(x in col for x in ['Edad', 'Genero', 'Pais', 'Nivel', 'Categoria'])][:6]\n",
        "\n",
        "if columnas_mostrar:\n",
        "    muestra = df_clean[columnas_mostrar].head(3)\n",
        "    for i, (idx, row) in enumerate(muestra.iterrows()):\n",
        "        print(f\"\\n   Fila {i+1}:\")\n",
        "        for col in columnas_mostrar:\n",
        "            valor = str(row[col])[:30] + \"...\" if len(str(row[col])) > 30 else str(row[col])\n",
        "            print(f\"     ‚Ä¢ {col}: {valor}\")\n",
        "\n",
        "print(f\"\\nüéâ TRANSFORMACI√ìN COMPLETADA EXITOSAMENTE\")\n",
        "print(f\"üìä Dataset listo para carga y an√°lisis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Carga de Datos\n",
        "\n",
        "### Descripci√≥n del Proceso de Carga\n",
        "\n",
        "La carga de datos es la fase final del proceso ETL, donde exportamos los datos limpios y transformados en diferentes formatos para su posterior an√°lisis y uso. Esta fase asegura que los datos est√©n disponibles en los formatos m√°s apropiados para diferentes tipos de an√°lisis y herramientas.\n",
        "\n",
        "### Formatos de Exportaci√≥n\n",
        "\n",
        "Exportaremos los datos en m√∫ltiples formatos para maximizar su utilidad:\n",
        "\n",
        "1. **CSV**: Formato universal para an√°lisis en Python, R y otras herramientas\n",
        "2. **Excel**: Formato ideal para presentaciones y an√°lisis en herramientas de BI\n",
        "3. **SQLite**: Base de datos ligera para consultas SQL y an√°lisis relacional\n",
        "4. **Reportes JSON**: Metadatos y m√©tricas de calidad en formato estructurado\n",
        "5. **Log CSV**: Registro detallado de todas las transformaciones aplicadas\n",
        "\n",
        "### Validaci√≥n de la Carga\n",
        "\n",
        "Despu√©s de cada exportaci√≥n, verificaremos que los datos se hayan guardado correctamente comparando el n√∫mero de filas y columnas, y validando la integridad de los datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Proceso de carga de datos\n",
        "print(\"üíæ PROCESO DE CARGA DE DATOS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Crear directorio de salida si no existe\n",
        "import os\n",
        "os.makedirs('/mnt/data', exist_ok=True)\n",
        "\n",
        "# 1. Exportar a CSV\n",
        "print(\"\\nüìÑ EXPORTACI√ìN A CSV:\")\n",
        "csv_filename = '/mnt/data/multipleChoiceResponses_clean.csv'\n",
        "\n",
        "try:\n",
        "    df_clean.to_csv(csv_filename, index=False, encoding='utf-8')\n",
        "    csv_size = os.path.getsize(csv_filename) / 1024  # KB\n",
        "    \n",
        "    print(f\"   ‚úÖ CSV creado: {csv_filename}\")\n",
        "    print(f\"   üìä Registros: {df_clean.shape[0]:,}\")\n",
        "    print(f\"   üìã Columnas: {df_clean.shape[1]:,}\")\n",
        "    print(f\"   üíæ Tama√±o: {csv_size:.1f} KB\")\n",
        "    \n",
        "    # Verificar carga\n",
        "    df_verificacion = pd.read_csv(csv_filename)\n",
        "    if df_verificacion.shape == df_clean.shape:\n",
        "        print(f\"   ‚úÖ Verificaci√≥n exitosa: {df_verificacion.shape[0]:,} filas √ó {df_verificacion.shape[1]:,} columnas\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå Error en verificaci√≥n\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error: {str(e)}\")\n",
        "\n",
        "# 2. Exportar a Excel\n",
        "print(\"\\nüìä EXPORTACI√ìN A EXCEL:\")\n",
        "excel_filename = '/mnt/data/multipleChoiceResponses_clean.xlsx'\n",
        "\n",
        "try:\n",
        "    with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
        "        # Hoja principal con datos\n",
        "        df_clean.to_excel(writer, sheet_name='Datos_Limpios', index=False)\n",
        "        \n",
        "        # Hoja con resumen\n",
        "        resumen = pd.DataFrame({\n",
        "            'M√©trica': ['Filas', 'Columnas', 'Valores_Nulos', 'Memoria_MB'],\n",
        "            'Valor': [df_clean.shape[0], df_clean.shape[1], \n",
        "                     df_clean.isnull().sum().sum(),\n",
        "                     df_clean.memory_usage(deep=True).sum() / 1024**2]\n",
        "        })\n",
        "        resumen.to_excel(writer, sheet_name='Resumen', index=False)\n",
        "    \n",
        "    excel_size = os.path.getsize(excel_filename) / 1024  # KB\n",
        "    print(f\"   ‚úÖ Excel creado: {excel_filename}\")\n",
        "    print(f\"   üìÑ Hojas: Datos_Limpios, Resumen\")\n",
        "    print(f\"   üíæ Tama√±o: {excel_size:.1f} KB\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error Excel: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Exportar a SQLite\n",
        "print(\"\\nüóÑÔ∏è EXPORTACI√ìN A SQLITE:\")\n",
        "sqlite_filename = '/mnt/data/etl_results.db'\n",
        "\n",
        "try:\n",
        "    # Crear conexi√≥n a SQLite\n",
        "    engine = sqlalchemy.create_engine(f'sqlite:///{sqlite_filename}')\n",
        "    \n",
        "    # Exportar datos principales\n",
        "    df_clean.to_sql('datos_limpios', engine, if_exists='replace', index=False)\n",
        "    \n",
        "    # Crear tabla de metadatos\n",
        "    metadatos = pd.DataFrame({\n",
        "        'metrica': ['fecha_procesamiento', 'filas_originales', 'filas_finales', 'columnas_originales', 'columnas_finales'],\n",
        "        'valor': [datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), \n",
        "                 filas_iniciales, filas_finales, columnas_iniciales, columnas_finales]\n",
        "    })\n",
        "    metadatos.to_sql('metadatos', engine, if_exists='replace', index=False)\n",
        "    \n",
        "    # Crear tabla de log de transformaciones\n",
        "    log_df = pd.DataFrame(transformation_log)\n",
        "    log_df.to_sql('log_transformaciones', engine, if_exists='replace', index=False)\n",
        "    \n",
        "    sqlite_size = os.path.getsize(sqlite_filename) / 1024  # KB\n",
        "    print(f\"   ‚úÖ SQLite creado: {sqlite_filename}\")\n",
        "    print(f\"   üìä Tablas: datos_limpios, metadatos, log_transformaciones\")\n",
        "    print(f\"   üíæ Tama√±o: {sqlite_size:.1f} KB\")\n",
        "    \n",
        "    # Verificar carga\n",
        "    with engine.connect() as conn:\n",
        "        result = conn.execute(sqlalchemy.text(\"SELECT COUNT(*) FROM datos_limpios\")).fetchone()\n",
        "        print(f\"   ‚úÖ Verificaci√≥n: {result[0]:,} registros en base de datos\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error SQLite: {str(e)}\")\n",
        "\n",
        "# 4. Crear reporte de calidad JSON\n",
        "print(\"\\nüìã CREACI√ìN DE REPORTE DE CALIDAD:\")\n",
        "json_filename = 'data_quality_report.json'\n",
        "\n",
        "try:\n",
        "    # Calcular m√©tricas de calidad\n",
        "    calidad_report = {\n",
        "        'fecha_procesamiento': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'dataset_original': {\n",
        "            'filas': int(filas_iniciales),\n",
        "            'columnas': int(columnas_iniciales),\n",
        "            'valores_nulos': int(df.isnull().sum().sum()),\n",
        "            'duplicados': int(df.duplicated().sum()),\n",
        "            'memoria_mb': float(df.memory_usage(deep=True).sum() / 1024**2)\n",
        "        },\n",
        "        'dataset_limpio': {\n",
        "            'filas': int(filas_finales),\n",
        "            'columnas': int(columnas_finales),\n",
        "            'valores_nulos': int(nulos_finales),\n",
        "            'duplicados': 0,\n",
        "            'memoria_mb': float(memoria_final)\n",
        "        },\n",
        "        'mejoras': {\n",
        "            'reduccion_nulos_porcentaje': float(reduccion_nulos),\n",
        "            'reduccion_memoria_porcentaje': float(reduccion_memoria),\n",
        "            'completitud_datos_porcentaje': float(((filas_finales * columnas_finales - nulos_finales) / (filas_finales * columnas_finales) * 100))\n",
        "        },\n",
        "        'transformaciones_aplicadas': [\n",
        "            'Eliminacion_Duplicados',\n",
        "            'Manejo_Valores_Nulos',\n",
        "            'Normalizacion_Strings',\n",
        "            'Renombrado_Columnas',\n",
        "            'Creacion_Variables_Derivadas'\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(calidad_report, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    json_size = os.path.getsize(json_filename) / 1024  # KB\n",
        "    print(f\"   ‚úÖ Reporte JSON creado: {json_filename}\")\n",
        "    print(f\"   üíæ Tama√±o: {json_size:.1f} KB\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error JSON: {str(e)}\")\n",
        "\n",
        "# 5. Crear log de transformaciones CSV\n",
        "print(\"\\nüìù CREACI√ìN DE LOG DE TRANSFORMACIONES:\")\n",
        "log_filename = 'transformation_log.csv'\n",
        "\n",
        "try:\n",
        "    log_df = pd.DataFrame(transformation_log)\n",
        "    log_df.to_csv(log_filename, index=False, encoding='utf-8')\n",
        "    \n",
        "    log_size = os.path.getsize(log_filename) / 1024  # KB\n",
        "    print(f\"   ‚úÖ Log CSV creado: {log_filename}\")\n",
        "    print(f\"   üìä Operaciones registradas: {len(transformation_log)}\")\n",
        "    print(f\"   üíæ Tama√±o: {log_size:.1f} KB\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error Log: {str(e)}\")\n",
        "\n",
        "print(f\"\\nüéâ CARGA COMPLETADA EXITOSAMENTE\")\n",
        "print(f\"üìÅ Archivos listos para an√°lisis y validaci√≥n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Comparaci√≥n con Power BI (Power Query)\n",
        "\n",
        "### Descripci√≥n de la Validaci√≥n Cruzada\n",
        "\n",
        "Para asegurar la consistencia y reproducibilidad del proceso ETL, es fundamental validar que los resultados obtenidos en Python puedan ser replicados en Power BI usando Power Query. Esta validaci√≥n cruzada garantiza que el proceso sea robusto y que los datos puedan ser procesados de manera consistente en diferentes herramientas.\n",
        "\n",
        "### Estrategia de Validaci√≥n\n",
        "\n",
        "La validaci√≥n se realizar√° mediante:\n",
        "\n",
        "1. **Script Power Query M**: C√≥digo que replique las transformaciones principales\n",
        "2. **M√©tricas de Comparaci√≥n**: Valores de referencia para verificar consistencia\n",
        "3. **Instrucciones de Implementaci√≥n**: Gu√≠a paso a paso para replicar en Power BI\n",
        "\n",
        "### Transformaciones a Replicar\n",
        "\n",
        "Las siguientes transformaciones ser√°n replicadas en Power Query:\n",
        "\n",
        "- Eliminaci√≥n de duplicados\n",
        "- Manejo de valores nulos\n",
        "- Normalizaci√≥n de strings\n",
        "- Renombrado de columnas principales\n",
        "- Creaci√≥n de variables derivadas b√°sicas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generar script Power Query M\n",
        "print(\"üîß GENERANDO SCRIPT POWER QUERY M\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Script Power Query M para replicar las transformaciones\n",
        "powerquery_script = f\"\"\"\n",
        "// SCRIPT POWER QUERY M - ETL KAGGLE SURVEY 2019\n",
        "// Replica las transformaciones realizadas en Python\n",
        "// Generado autom√°ticamente el {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "let\n",
        "    // PASO 1: Cargar datos desde CSV\n",
        "    Source = Csv.Document(File.Contents(\"multipleChoiceResponses.csv\"),\n",
        "        [Delimiter=\",\", Columns={df.shape[1]}, Encoding=65001, QuoteStyle=QuoteStyle.Csv]),\n",
        "    \n",
        "    // PASO 2: Promover encabezados\n",
        "    #\"Promoted Headers\" = Table.PromoteHeaders(Source, [PromoteAllScalars=true]),\n",
        "    \n",
        "    // PASO 3: Eliminar duplicados\n",
        "    #\"Removed Duplicates\" = Table.Distinct(#\"Promoted Headers\"),\n",
        "    \n",
        "    // PASO 4: Eliminar columnas con >80% nulos\n",
        "    // (Nota: Agregar columnas espec√≠ficas identificadas en Python)\n",
        "    #\"Removed High Null Columns\" = #\"Removed Duplicates\",\n",
        "    \n",
        "    // PASO 5: Reemplazar valores nulos\n",
        "    #\"Replaced Nulls\" = Table.ReplaceValue(#\"Removed High Null Columns\", null, \"No especificado\", \n",
        "        Replacer.ReplaceValue, Table.SelectColumns(#\"Removed High Null Columns\", \n",
        "        Table.ColumnsOfType(#\"Removed High Null Columns\", {{type text}}))),\n",
        "    \n",
        "    // PASO 6: Normalizar strings (min√∫sculas y espacios)\n",
        "    #\"Normalized Strings\" = Table.TransformColumns(#\"Replaced Nulls\", \n",
        "        {{\"Q1\", Text.Lower, type text},\n",
        "         {\"Q2\", Text.Lower, type text},\n",
        "         {\"Q3\", Text.Lower, type text},\n",
        "         {\"Q4\", Text.Lower, type text},\n",
        "         {\"Q5\", Text.Lower, type text}}),\n",
        "    \n",
        "    // PASO 7: Renombrar columnas principales\n",
        "    #\"Renamed Columns\" = Table.RenameColumns(#\"Normalized Strings\", {{\n",
        "        \"Q1\", \"Edad_Encuestado\",\n",
        "        \"Q2\", \"Genero\", \n",
        "        \"Q3\", \"Pais_Residencia\",\n",
        "        \"Q4\", \"Nivel_Educativo\",\n",
        "        \"Q5\", \"Area_Estudios_Principal\",\n",
        "        \"Q6\", \"Situacion_Laboral_Actual\",\n",
        "        \"Q7\", \"Cargo_Principal_Trabajo\",\n",
        "        \"Q8\", \"Anos_Experiencia_Campo\",\n",
        "        \"Q9\", \"Rango_Salarial_Anual\",\n",
        "        \"Q10\", \"Lenguajes_Programacion_Usados\"\n",
        "    }}),\n",
        "    \n",
        "    // PASO 8: Crear variables derivadas\n",
        "    #\"Added Categoria_Experiencia\" = Table.AddColumn(#\"Renamed Columns\", \"Categoria_Experiencia\", \n",
        "        each if Text.Contains(Text.Lower([Anos_Experiencia_Campo] ?? \"\"), \"0-1\") then \"principiante (0-2 a√±os)\"\n",
        "             else if Text.Contains(Text.Lower([Anos_Experiencia_Campo] ?? \"\"), \"10\") then \"experto (10+ a√±os)\"\n",
        "             else if Text.Contains(Text.Lower([Anos_Experiencia_Campo] ?? \"\"), \"5\") then \"avanzado (5-10 a√±os)\"\n",
        "             else \"intermedio (3-5 a√±os)\"),\n",
        "    \n",
        "    #\"Added Categoria_Salarial\" = Table.AddColumn(#\"Added Categoria_Experiencia\", \"Categoria_Salarial\",\n",
        "        each if Text.Contains(Text.Lower([Rango_Salarial_Anual] ?? \"\"), \"0-10\") then \"bajo (0-20k usd)\"\n",
        "             else if Text.Contains(Text.Lower([Rango_Salarial_Anual] ?? \"\"), \"20-50\") then \"medio (20-50k usd)\"\n",
        "             else if Text.Contains(Text.Lower([Rango_Salarial_Anual] ?? \"\"), \"50-80\") then \"alto (50-80k usd)\"\n",
        "             else if Text.Contains(Text.Lower([Rango_Salarial_Anual] ?? \"\"), \"100\") then \"ejecutivo (100k+ usd)\"\n",
        "             else \"no especificado\"),\n",
        "    \n",
        "    // RESULTADO FINAL\n",
        "    #\"Final Result\" = #\"Added Categoria_Salarial\"\n",
        "in\n",
        "    #\"Final Result\"\n",
        "\"\"\"\n",
        "\n",
        "# Guardar script Power Query\n",
        "powerquery_filename = 'powerquery_replica.pq'\n",
        "try:\n",
        "    with open(powerquery_filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(powerquery_script)\n",
        "    \n",
        "    powerquery_size = os.path.getsize(powerquery_filename) / 1024  # KB\n",
        "    print(f\"‚úÖ Script Power Query creado: {powerquery_filename}\")\n",
        "    print(f\"üíæ Tama√±o: {powerquery_size:.1f} KB\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {str(e)}\")\n",
        "\n",
        "# Generar m√©tricas de validaci√≥n\n",
        "print(f\"\\nüìä GENERANDO M√âTRICAS DE VALIDACI√ìN\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "metricas_validacion = {\n",
        "    'fecha_validacion': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "    'metricas_python': {\n",
        "        'total_registros': int(filas_finales),\n",
        "        'total_columnas': int(columnas_finales),\n",
        "        'valores_nulos': int(nulos_finales),\n",
        "        'memoria_mb': float(memoria_final),\n",
        "        'duplicados': 0\n",
        "    },\n",
        "    'criterios_validacion': {\n",
        "        'diferencia_registros_maxima': '1%',\n",
        "        'diferencia_columnas_maxima': '0',\n",
        "        'valores_nulos_maximos': 0,\n",
        "        'coincidencia_distribuciones_minima': '95%'\n",
        "    },\n",
        "    'instrucciones_validacion': [\n",
        "        '1. Importar multipleChoiceResponses.csv en Power BI',\n",
        "        '2. Aplicar el script M generado en powerquery_replica.pq',\n",
        "        '3. Comparar m√©tricas con valores de referencia',\n",
        "        '4. Validar distribuciones de variables principales',\n",
        "        '5. Generar dashboard con visualizaciones clave'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Guardar m√©tricas de validaci√≥n\n",
        "metricas_filename = 'metricas_validacion_powerbi.txt'\n",
        "try:\n",
        "    with open(metricas_filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"M√âTRICAS DE VALIDACI√ìN - PYTHON vs POWER BI\\n\")\n",
        "        f.write(\"=\"*60 + \"\\n\\n\")\n",
        "        f.write(f\"Fecha de validaci√≥n: {metricas_validacion['fecha_validacion']}\\n\\n\")\n",
        "        f.write(\"M√âTRICAS DE REFERENCIA (PYTHON):\\n\")\n",
        "        for metrica, valor in metricas_validacion['metricas_python'].items():\n",
        "            f.write(f\"‚Ä¢ {metrica}: {valor}\\n\")\n",
        "        f.write(\"\\nCRITERIOS DE VALIDACI√ìN EXITOSA:\\n\")\n",
        "        for criterio, valor in metricas_validacion['criterios_validacion'].items():\n",
        "            f.write(f\"‚Ä¢ {criterio}: {valor}\\n\")\n",
        "        f.write(\"\\nINSTRUCCIONES DE VALIDACI√ìN:\\n\")\n",
        "        for instruccion in metricas_validacion['instrucciones_validacion']:\n",
        "            f.write(f\"{instruccion}\\n\")\n",
        "    \n",
        "    metricas_size = os.path.getsize(metricas_filename) / 1024  # KB\n",
        "    print(f\"‚úÖ M√©tricas de validaci√≥n creadas: {metricas_filename}\")\n",
        "    print(f\"üíæ Tama√±o: {metricas_size:.1f} KB\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {str(e)}\")\n",
        "\n",
        "print(f\"\\nüéâ VALIDACI√ìN POWER BI PREPARADA\")\n",
        "print(f\"üìÅ Archivos listos para comparaci√≥n cruzada\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
