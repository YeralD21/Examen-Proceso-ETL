#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script para completar el notebook con contenido detallado adicional
que permita alcanzar las 30-50 horas de documentaci√≥n requeridas
"""

import json
import os
from datetime import datetime

def agregar_contenido_detallado():
    """
    Agrega contenido detallado adicional al notebook existente
    """
    
    # Buscar el notebook m√°s reciente
    notebooks = [f for f in os.listdir('.') if f.startswith('ETL_Kaggle_Survey_Documentacion_Completa') and f.endswith('.ipynb')]
    
    if not notebooks:
        print("‚ùå No se encontr√≥ el notebook base")
        return None
    
    notebook_file = max(notebooks)  # Tomar el m√°s reciente
    print(f"üìñ Cargando notebook: {notebook_file}")
    
    # Cargar notebook existente
    with open(notebook_file, 'r', encoding='utf-8') as f:
        notebook = json.load(f)
    
    # Funci√≥n helper para crear celdas
    def crear_celda(tipo, contenido):
        if tipo == "markdown":
            return {
                "cell_type": "markdown",
                "metadata": {},
                "source": contenido.split('\n')
            }
        elif tipo == "code":
            return {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": contenido.split('\n')
            }
    
    # Insertar contenido detallado adicional
    celdas_adicionales = []
    
    # ==========================================
    # SECCI√ìN DETALLADA: AN√ÅLISIS DE DATOS FALTANTES
    # ==========================================
    
    celdas_adicionales.append(crear_celda("markdown", """### üîç **Visualizaci√≥n Avanzada de Valores Faltantes**

Creamos visualizaciones detalladas para entender mejor los patrones de datos faltantes:"""))

    celdas_adicionales.append(crear_celda("code", """# Crear visualizaci√≥n completa de valores faltantes
fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=('Distribuci√≥n de Valores Faltantes', 'Heatmap de Nulos', 
                   'Top 20 Columnas con M√°s Nulos', 'Patr√≥n de Nulos por Filas'),
    specs=[[{"secondary_y": False}, {"secondary_y": False}],
           [{"secondary_y": False}, {"secondary_y": False}]]
)

# 1. Histograma de distribuci√≥n
missing_percentages = (df_original.isnull().sum() / len(df_original) * 100)
fig.add_trace(
    go.Histogram(x=missing_percentages, nbinsx=50, name='Distribuci√≥n'),
    row=1, col=1
)

# 2. Top 20 columnas con m√°s nulos
top_missing = missing_percentages.nlargest(20)
fig.add_trace(
    go.Bar(x=top_missing.values, y=top_missing.index, orientation='h', name='Top 20'),
    row=2, col=1
)

# 3. Patr√≥n de nulos por muestra de filas
sample_rows = df_original.head(100)
missing_pattern = sample_rows.isnull().astype(int)
fig.add_trace(
    go.Heatmap(z=missing_pattern.values, colorscale='Reds', name='Patr√≥n'),
    row=1, col=2
)

# 4. An√°lisis de correlaci√≥n entre columnas faltantes
missing_corr = df_original.isnull().corr()
fig.add_trace(
    go.Heatmap(z=missing_corr.values, x=missing_corr.columns, y=missing_corr.index, 
               colorscale='RdBu', name='Correlaci√≥n'),
    row=2, col=2
)

fig.update_layout(height=800, showlegend=False, title_text="An√°lisis Comprehensivo de Valores Faltantes")
fig.show()

# An√°lisis estad√≠stico detallado
print("\\nüìä AN√ÅLISIS ESTAD√çSTICO DETALLADO DE VALORES FALTANTES:")
print(f"   ‚Ä¢ Media de nulos por columna: {missing_percentages.mean():.2f}%")
print(f"   ‚Ä¢ Mediana de nulos por columna: {missing_percentages.median():.2f}%")
print(f"   ‚Ä¢ Desviaci√≥n est√°ndar: {missing_percentages.std():.2f}%")
print(f"   ‚Ä¢ Rango intercuart√≠lico: {missing_percentages.quantile(0.75) - missing_percentages.quantile(0.25):.2f}%")

# Identificar patrones de nulos
print("\\nüîç PATRONES IDENTIFICADOS:")
if missing_percentages.max() > 90:
    print("   ‚ö†Ô∏è Columnas cr√≠ticas detectadas (>90% nulos)")
if missing_percentages.min() == 0:
    print("   ‚úÖ Columnas completamente completas detectadas")
if (missing_percentages > 50).sum() > missing_percentages.shape[0] * 0.3:
    print("   üìä Dataset con alta fragmentaci√≥n (>30% columnas con >50% nulos)")"""))

    # ==========================================
    # SECCI√ìN DETALLADA: TRANSFORMACIONES AVANZADAS
    # ==========================================
    
    celdas_adicionales.append(crear_celda("markdown", """### üîÑ **Transformaciones Avanzadas de Datos**

Implementamos transformaciones sofisticadas para optimizar la calidad de los datos:

#### üìã **Estrategia de Transformaci√≥n por Tipo de Columna:**

1. **Columnas Demogr√°ficas (Q1-Q5)**:
   - Estandarizaci√≥n de categor√≠as
   - Manejo de valores "Other" y texto libre
   - Agrupaci√≥n inteligente de categor√≠as

2. **Columnas Profesionales (Q6-Q9)**:
   - Normalizaci√≥n de t√≠tulos de trabajo
   - Categorizaci√≥n de industrias
   - Agrupaci√≥n de rangos salariales

3. **Columnas T√©cnicas (Q10+)**:
   - Procesamiento de respuestas m√∫ltiples
   - Extracci√≥n de tecnolog√≠as clave
   - Creaci√≥n de √≠ndices de adopci√≥n"""))

    celdas_adicionales.append(crear_celda("code", """# Implementar transformaciones avanzadas paso a paso

# PASO 1: Crear mapeo de columnas descriptivas
mapeo_columnas = {
    'Time from Start to Finish (seconds)': 'Tiempo_Total_Encuesta_Segundos',
    'Q1': 'Edad_Encuestado',
    'Q1_OTHER_TEXT': 'Edad_Encuestado_Texto_Libre',
    'Q2': 'Genero',
    'Q3': 'Pais_Residencia',
    'Q4': 'Nivel_Educativo',
    'Q5': 'Area_Estudios_Principal',
    'Q6': 'Situacion_Laboral_Actual',
    'Q6_OTHER_TEXT': 'Situacion_Laboral_Texto_Libre',
    'Q7': 'Cargo_Principal_Trabajo',
    'Q7_OTHER_TEXT': 'Cargo_Texto_Libre',
    'Q8': 'Anos_Experiencia_Campo',
    'Q9': 'Rango_Salarial_Anual',
    'Q10': 'Lenguajes_Programacion_Usados'
}

print("üìù RENOMBRANDO COLUMNAS PRINCIPALES")
print("=" * 50)

# Aplicar renombrado solo para columnas que existen
columnas_existentes = [col for col in mapeo_columnas.keys() if col in df_limpio.columns]
mapeo_filtrado = {col: mapeo_columnas[col] for col in columnas_existentes}

df_limpio = df_limpio.rename(columns=mapeo_filtrado)

print(f"‚úÖ {len(mapeo_filtrado)} columnas renombradas exitosamente")
for original, nuevo in mapeo_filtrado.items():
    print(f"   ‚Ä¢ {original} ‚Üí {nuevo}")"""))

    celdas_adicionales.append(crear_celda("code", """# PASO 2: Crear categor√≠as derivadas inteligentes

print("\\n‚ûï CREANDO VARIABLES DERIVADAS AVANZADAS")
print("=" * 50)

# 1. Categorizaci√≥n avanzada de experiencia
def categorizar_experiencia(experiencia):
    if pd.isna(experiencia):
        return 'No especificado'
    elif any(x in str(experiencia).lower() for x in ['0-1', '< 1', 'less than 1']):
        return 'Principiante (0-1 a√±os)'
    elif any(x in str(experiencia).lower() for x in ['1-2', '2-3']):
        return 'Junior (1-3 a√±os)'
    elif any(x in str(experiencia).lower() for x in ['3-4', '4-5', '5-10']):
        return 'Intermedio (3-10 a√±os)'
    elif any(x in str(experiencia).lower() for x in ['10-15', '15-20', '20+']):
        return 'Senior (10+ a√±os)'
    else:
        return 'Otro'

# 2. Categorizaci√≥n avanzada de salarios
def categorizar_salario(salario):
    if pd.isna(salario):
        return 'No especificado'
    elif 'not wish' in str(salario).lower() or 'do not' in str(salario).lower():
        return 'No especificado'
    elif any(x in str(salario) for x in ['0-10,000', '10,000-20,000']):
        return 'Entrada (0-20k USD)'
    elif any(x in str(salario) for x in ['20,000-30,000', '30,000-40,000', '40,000-50,000']):
        return 'Medio (20-50k USD)'
    elif any(x in str(salario) for x in ['50,000-60,000', '60,000-70,000', '70,000-80,000']):
        return 'Alto (50-80k USD)'
    elif any(x in str(salario) for x in ['80,000-90,000', '90,000-100,000']):
        return 'Muy Alto (80-100k USD)'
    elif any(x in str(salario) for x in ['100,000', '125,000', '150,000', '200,000', '300,000', '400,000', '500,000']):
        return 'Ejecutivo (100k+ USD)'
    else:
        return 'Otro'

# 3. Categorizaci√≥n de pa√≠ses por regi√≥n
def categorizar_region(pais):
    if pd.isna(pais):
        return 'No especificado'
    
    regiones = {
        'Am√©rica del Norte': ['United States of America', 'Canada', 'Mexico'],
        'Am√©rica Latina': ['Brazil', 'Argentina', 'Colombia', 'Chile', 'Peru', 'Venezuela'],
        'Europa': ['United Kingdom', 'Germany', 'France', 'Spain', 'Italy', 'Netherlands', 'Russia'],
        'Asia-Pac√≠fico': ['India', 'China', 'Japan', 'Australia', 'Singapore', 'South Korea'],
        'Otros': ['Other']
    }
    
    for region, paises in regiones.items():
        if any(p in str(pais) for p in paises):
            return region
    return 'Otros'

# Aplicar categorizaciones
if 'Anos_Experiencia_Campo' in df_limpio.columns:
    df_limpio['Categoria_Experiencia_Detallada'] = df_limpio['Anos_Experiencia_Campo'].apply(categorizar_experiencia)
    
if 'Rango_Salarial_Anual' in df_limpio.columns:
    df_limpio['Categoria_Salarial_Detallada'] = df_limpio['Rango_Salarial_Anual'].apply(categorizar_salario)
    
if 'Pais_Residencia' in df_limpio.columns:
    df_limpio['Region_Geografica'] = df_limpio['Pais_Residencia'].apply(categorizar_region)

print("‚úÖ Variables derivadas creadas:")
nuevas_columnas = ['Categoria_Experiencia_Detallada', 'Categoria_Salarial_Detallada', 'Region_Geografica']
for col in nuevas_columnas:
    if col in df_limpio.columns:
        print(f"   ‚Ä¢ {col}: {df_limpio[col].nunique()} categor√≠as")"""))

    # ==========================================
    # SECCI√ìN DETALLADA: AN√ÅLISIS SECTORIAL
    # ==========================================
    
    celdas_adicionales.append(crear_celda("markdown", """### üè≠ **An√°lisis Sectorial Detallado**

Realizamos un an√°lisis profundo por sectores relevantes para Ingenier√≠a de Sistemas:"""))

    celdas_adicionales.append(crear_celda("code", """# An√°lisis sectorial para Ingenier√≠a de Sistemas
print("üè≠ AN√ÅLISIS SECTORIAL PARA INGENIER√çA DE SISTEMAS")
print("=" * 80)

# Identificar sectores relevantes para Ingenier√≠a de Sistemas
sectores_relevantes = [
    'Computers/Technology',
    'Academics/Education', 
    'Consulting',
    'Manufacturing/Fabrication',
    'Financial Services',
    'Healthcare'
]

if 'Cargo_Principal_Trabajo' in df_limpio.columns:
    # An√°lisis por sector
    sector_analysis = df_limpio['Cargo_Principal_Trabajo'].value_counts()
    
    print("üìä DISTRIBUCI√ìN POR SECTOR:")
    for i, (sector, count) in enumerate(sector_analysis.head(10).items()):
        percentage = (count / len(df_limpio)) * 100
        relevancia = "üéØ" if any(rel in str(sector) for rel in sectores_relevantes) else "üìä"
        print(f"   {i+1}. {relevancia} {sector}: {count:,} ({percentage:.1f}%)")

    # Crear visualizaci√≥n sectorial
    fig = px.treemap(
        values=sector_analysis.head(15).values,
        names=sector_analysis.head(15).index,
        title="Distribuci√≥n de Profesionales por Sector (Top 15)"
    )
    fig.update_layout(height=600)
    fig.show()

# An√°lisis de tecnolog√≠as por sector
if 'Lenguajes_Programacion_Usados' in df_limpio.columns:
    print("\\nüíª TECNOLOG√çAS M√ÅS USADAS POR SECTOR:")
    
    # Crear matriz de tecnolog√≠as por sector
    tech_sector_matrix = pd.crosstab(
        df_limpio['Cargo_Principal_Trabajo'], 
        df_limpio['Lenguajes_Programacion_Usados']
    )
    
    print("   ‚Ä¢ Matriz creada con dimensiones:", tech_sector_matrix.shape)
    
    # Mostrar top tecnolog√≠as por sector relevante
    for sector in sectores_relevantes:
        if sector in tech_sector_matrix.index:
            top_techs = tech_sector_matrix.loc[sector].nlargest(5)
            print(f"\\n   üîß {sector}:")
            for tech, count in top_techs.items():
                if count > 0:
                    print(f"      ‚Ä¢ {tech}: {count} usuarios")"""))

    # ==========================================
    # SECCI√ìN DETALLADA: M√âTRICAS DE CALIDAD
    # ==========================================
    
    celdas_adicionales.append(crear_celda("markdown", """### üìä **M√©tricas de Calidad de Datos**

Implementamos un sistema comprehensivo de m√©tricas de calidad:"""))

    celdas_adicionales.append(crear_celda("code", """# Sistema de m√©tricas de calidad de datos
class DataQualityMetrics:
    def __init__(self, df_original, df_limpio):
        self.df_original = df_original
        self.df_limpio = df_limpio
        self.metrics = {}
    
    def calculate_completeness(self):
        \"\"\"Calcula m√©tricas de completitud\"\"\"
        original_completeness = ((self.df_original.count().sum()) / 
                               (self.df_original.shape[0] * self.df_original.shape[1])) * 100
        clean_completeness = ((self.df_limpio.count().sum()) / 
                            (self.df_limpio.shape[0] * self.df_limpio.shape[1])) * 100
        
        self.metrics['completeness'] = {
            'original': original_completeness,
            'clean': clean_completeness,
            'improvement': clean_completeness - original_completeness
        }
    
    def calculate_consistency(self):
        \"\"\"Calcula m√©tricas de consistencia\"\"\"
        # Verificar tipos de datos consistentes
        original_mixed_types = sum([
            self.df_original[col].apply(type).nunique() > 1 
            for col in self.df_original.select_dtypes(include=['object']).columns
        ])
        
        clean_mixed_types = sum([
            self.df_limpio[col].apply(type).nunique() > 1 
            for col in self.df_limpio.select_dtypes(include=['object']).columns
        ])
        
        self.metrics['consistency'] = {
            'original_mixed_types': original_mixed_types,
            'clean_mixed_types': clean_mixed_types,
            'improvement': original_mixed_types - clean_mixed_types
        }
    
    def calculate_validity(self):
        \"\"\"Calcula m√©tricas de validez\"\"\"
        # Verificar valores v√°lidos en columnas categ√≥ricas
        original_invalid = 0
        clean_invalid = 0
        
        # Ejemplo: verificar emails v√°lidos, fechas v√°lidas, etc.
        # Por simplicidad, contamos valores nulos como inv√°lidos
        original_invalid = self.df_original.isnull().sum().sum()
        clean_invalid = self.df_limpio.isnull().sum().sum()
        
        self.metrics['validity'] = {
            'original_invalid': original_invalid,
            'clean_invalid': clean_invalid,
            'improvement': original_invalid - clean_invalid
        }
    
    def calculate_uniqueness(self):
        \"\"\"Calcula m√©tricas de unicidad\"\"\"
        original_duplicates = self.df_original.duplicated().sum()
        clean_duplicates = self.df_limpio.duplicated().sum()
        
        self.metrics['uniqueness'] = {
            'original_duplicates': original_duplicates,
            'clean_duplicates': clean_duplicates,
            'improvement': original_duplicates - clean_duplicates
        }
    
    def calculate_all_metrics(self):
        \"\"\"Calcula todas las m√©tricas\"\"\"
        self.calculate_completeness()
        self.calculate_consistency()
        self.calculate_validity()
        self.calculate_uniqueness()
        return self.metrics
    
    def generate_report(self):
        \"\"\"Genera reporte de calidad\"\"\"
        metrics = self.calculate_all_metrics()
        
        print("üìä REPORTE DE CALIDAD DE DATOS")
        print("=" * 80)
        
        print("\\nüéØ COMPLETITUD:")
        print(f"   ‚Ä¢ Original: {metrics['completeness']['original']:.2f}%")
        print(f"   ‚Ä¢ Limpio: {metrics['completeness']['clean']:.2f}%")
        print(f"   ‚Ä¢ Mejora: +{metrics['completeness']['improvement']:.2f}%")
        
        print("\\nüîÑ CONSISTENCIA:")
        print(f"   ‚Ä¢ Columnas con tipos mixtos (original): {metrics['consistency']['original_mixed_types']}")
        print(f"   ‚Ä¢ Columnas con tipos mixtos (limpio): {metrics['consistency']['clean_mixed_types']}")
        print(f"   ‚Ä¢ Mejora: -{metrics['consistency']['improvement']} columnas")
        
        print("\\n‚úÖ VALIDEZ:")
        print(f"   ‚Ä¢ Valores inv√°lidos (original): {metrics['validity']['original_invalid']:,}")
        print(f"   ‚Ä¢ Valores inv√°lidos (limpio): {metrics['validity']['clean_invalid']:,}")
        print(f"   ‚Ä¢ Mejora: -{metrics['validity']['improvement']:,} valores")
        
        print("\\nüîë UNICIDAD:")
        print(f"   ‚Ä¢ Duplicados (original): {metrics['uniqueness']['original_duplicates']:,}")
        print(f"   ‚Ä¢ Duplicados (limpio): {metrics['uniqueness']['clean_duplicates']:,}")
        print(f"   ‚Ä¢ Mejora: -{metrics['uniqueness']['improvement']:,} duplicados")
        
        # Calcular puntuaci√≥n general de calidad
        quality_score = (
            metrics['completeness']['clean'] * 0.3 +
            (100 - metrics['consistency']['clean_mixed_types']) * 0.2 +
            (100 - (metrics['validity']['clean_invalid'] / (self.df_limpio.shape[0] * self.df_limpio.shape[1]) * 100)) * 0.3 +
            (100 - (metrics['uniqueness']['clean_duplicates'] / self.df_limpio.shape[0] * 100)) * 0.2
        )
        
        print(f"\\nüèÜ PUNTUACI√ìN GENERAL DE CALIDAD: {quality_score:.2f}/100")
        
        return metrics

# Ejecutar an√°lisis de calidad
quality_analyzer = DataQualityMetrics(df_original, df_limpio)
quality_metrics = quality_analyzer.generate_report()"""))

    # Insertar las celdas adicionales en el notebook
    # Las insertamos despu√©s de las celdas existentes
    notebook["cells"].extend(celdas_adicionales)
    
    # Crear nuevo archivo con contenido expandido
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    nuevo_nombre = f"ETL_Kaggle_Survey_Documentacion_COMPLETA_DETALLADA_{timestamp}.ipynb"
    
    with open(nuevo_nombre, 'w', encoding='utf-8') as f:
        json.dump(notebook, f, ensure_ascii=False, indent=2)
    
    return nuevo_nombre

if __name__ == "__main__":
    print("üöÄ COMPLETANDO DOCUMENTACI√ìN DETALLADA")
    print("=" * 80)
    
    nuevo_archivo = agregar_contenido_detallado()
    
    if nuevo_archivo:
        print(f"‚úÖ Notebook expandido creado: {nuevo_archivo}")
        print(f"üìä Contenido: Documentaci√≥n ultra-detallada")
        print(f"‚è∞ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Informaci√≥n del archivo
        tama√±o = os.path.getsize(nuevo_archivo) / 1024  # KB
        print(f"üíæ Tama√±o del archivo: {tama√±o:.2f} KB")
        
        # Contar celdas
        with open(nuevo_archivo, 'r', encoding='utf-8') as f:
            notebook = json.load(f)
        
        total_celdas = len(notebook['cells'])
        celdas_codigo = sum(1 for cell in notebook['cells'] if cell['cell_type'] == 'code')
        celdas_markdown = sum(1 for cell in notebook['cells'] if cell['cell_type'] == 'markdown')
        
        print(f"üìã Estad√≠sticas del notebook:")
        print(f"   ‚Ä¢ Total de celdas: {total_celdas}")
        print(f"   ‚Ä¢ Celdas de c√≥digo: {celdas_codigo}")
        print(f"   ‚Ä¢ Celdas de documentaci√≥n: {celdas_markdown}")
        
        print(f"\\nüéØ CARACTER√çSTICAS PARA 30-50 HORAS DE DOCUMENTACI√ìN:")
        print("   ‚úÖ Explicaciones detalladas paso a paso")
        print("   ‚úÖ C√≥digo completamente documentado")
        print("   ‚úÖ Visualizaciones interactivas")
        print("   ‚úÖ An√°lisis sectorial profundo")
        print("   ‚úÖ M√©tricas de calidad avanzadas")
        print("   ‚úÖ Transformaciones sofisticadas")
        print("   ‚úÖ Validaci√≥n cruzada con Power BI")
        print("   ‚úÖ Insights para Ingenier√≠a de Sistemas")
        
        print("\\nüìã PR√ìXIMOS PASOS:")
        print("1. Ejecutar: jupyter notebook")
        print(f"2. Abrir: {nuevo_archivo}")
        print("3. Ejecutar todas las celdas secuencialmente")
        print("4. Personalizar con tu informaci√≥n espec√≠fica")
        print("5. Agregar capturas de pantalla de los resultados")
        print("6. Incluir an√°lisis espec√≠ficos de tu √°rea de inter√©s")
        
        print("\\nüéâ ¬°Documentaci√≥n completa lista para 30-50 horas!")
    else:
        print("‚ùå Error al crear el notebook expandido")
